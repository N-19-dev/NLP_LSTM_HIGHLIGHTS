{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: kaggle in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (1.6.17)\n",
      "Requirement already satisfied: numpy in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: tqdm in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: bleach in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: webencodings in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow kaggle numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des datasets\n",
    "train_data = pd.read_csv(\"/Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/TP1_LSTM/cnn_dailymail/train.csv\")\n",
    "val_data = pd.read_csv(\"/Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/TP1_LSTM/cnn_dailymail/validation.csv\")\n",
    "test_data = pd.read_csv(\"/Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/TP1_LSTM/cnn_dailymail/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les données pour chaque ensemble\n",
    "def prepare_data(data, tokenizer=None, fit_tokenizer=False):\n",
    "    articles = data['article'].values\n",
    "    summaries = data['highlights'].values\n",
    "    \n",
    "    if fit_tokenizer:\n",
    "        tokenizer.fit_on_texts(articles)\n",
    "\n",
    "    # Convertir les articles et les résumés en séquences\n",
    "    articles_sequences = tokenizer.texts_to_sequences(articles)\n",
    "    summaries_sequences = tokenizer.texts_to_sequences(summaries)\n",
    "    \n",
    "    # Compléter les séquences pour les rendre de longueur égale\n",
    "    articles_padded = pad_sequences(articles_sequences, maxlen=max_len, padding='post')\n",
    "    summaries_padded = pad_sequences(summaries_sequences, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Créer les étiquettes pour chaque mot (moment fort / non-moment fort)\n",
    "    labels = np.zeros_like(articles_padded)\n",
    "    for i, summary in enumerate(summaries_padded):\n",
    "        for word in summary:\n",
    "            if word != 0:\n",
    "                labels[i, np.where(articles_padded[i] == word)] = 1\n",
    "    \n",
    "    return articles_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les paramètres du modèle\n",
    "vocab_size = 20000  # Nombre maximum de mots à garder\n",
    "max_len = 100  # Longueur maximale des séquences\n",
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer le tokenizer et préparer les données\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "X_train, y_train = prepare_data(train_data, tokenizer=tokenizer, fit_tokenizer=True)\n",
    "X_val, y_val = prepare_data(val_data, tokenizer=tokenizer)\n",
    "X_test, y_test = prepare_data(test_data, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathansornet/Documents/Cours/INGE_3/ESME/NLP/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Construction du modèle LSTM\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.7532 - loss: 0.5472 - val_accuracy: 0.8135 - val_loss: 0.4360\n",
      "Epoch 2/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 75ms/step - accuracy: 0.8188 - loss: 0.4183 - val_accuracy: 0.8135 - val_loss: 0.4324\n",
      "Epoch 3/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 75ms/step - accuracy: 0.8206 - loss: 0.4014 - val_accuracy: 0.8112 - val_loss: 0.4382\n",
      "Epoch 4/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 77ms/step - accuracy: 0.8241 - loss: 0.3904 - val_accuracy: 0.8105 - val_loss: 0.4442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3626a5ee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle avec validation\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Entraînement du modèle avec validation et early stopping\n",
    "model.fit(X_train[:5000], y_train[:5000],  # Utilisez un sous-échantillon des données (par exemple, 5000 échantillons)\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.8155 - loss: 0.4285\n",
      "Loss: 0.42816779017448425, Accuracy: 0.8156216144561768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation du modèle\n",
    "test_article = \"Texte d'exemple pour identifier les moments forts.\"\n",
    "test_sequence = tokenizer.texts_to_sequences([test_article])\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Moments forts prédits: [[[1]\n",
      "  [1]\n",
      "  [0]\n",
      "  [1]\n",
      "  [0]\n",
      "  [0]\n",
      "  [1]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_padded)\n",
    "predicted_moments = (predictions > 0.5).astype(int)\n",
    "\n",
    "print(\"Moments forts prédits:\", predicted_moments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Texte avec moments forts : (example) (text) to (identify) (highlights) (this) (project) (uses) (machine) (learning) to (extract) (key) (information) (from) (texts) (highlights) (represent) the (most) (important) (sentences) (or) (parts)\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'article à tester\n",
    "test_article = \"Example text to identify highlights. This project uses machine learning to extract key information from texts. Highlights represent the most important sentences or parts.\"\n",
    "\n",
    "# Convertir le texte d'exemple en séquence\n",
    "test_sequence = tokenizer.texts_to_sequences([test_article])\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "# Prédire les moments forts\n",
    "predictions = model.predict(test_padded)[0]\n",
    "predicted_moments = (predictions > 0.5).astype(int)  # Moments forts là où la probabilité est > 0.5\n",
    "\n",
    "# Extraire et afficher les moments forts\n",
    "tokenizer.index_word = {index: word for word, index in tokenizer.word_index.items() if index < vocab_size}\n",
    "\n",
    "# Reconstruire les mots de l'article et identifier les moments forts\n",
    "highlighted_text = []\n",
    "for i, pred in enumerate(predicted_moments):\n",
    "    if pred == 1 and test_padded[0][i] != 0:\n",
    "        highlighted_text.append(tokenizer.index_word[test_padded[0][i]])\n",
    "    elif test_padded[0][i] != 0:\n",
    "        highlighted_text.append(f\"({tokenizer.index_word[test_padded[0][i]]})\")\n",
    "\n",
    "highlighted_text = ' '.join(highlighted_text)\n",
    "print(\"Texte avec moments forts :\", highlighted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
